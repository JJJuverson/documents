(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{287:function(a,t,s){a.exports=s.p+"assets/img/hash8.c482fd47.png"},359:function(a,t,s){a.exports=s.p+"assets/img/lunwen003.41f74a00.png"},360:function(a,t,s){a.exports=s.p+"assets/img/lunwen004.7847a33e.png"},361:function(a,t,s){a.exports=s.p+"assets/img/lunwen005.dcf7c3ae.png"},362:function(a,t,s){a.exports=s.p+"assets/img/lunwen006.24e8c25d.png"},437:function(a,t,s){"use strict";s.r(t);var e=s(3),r=Object(e.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h2",{attrs:{id:"摘要"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[a._v("#")]),a._v(" 摘要")]),a._v(" "),e("ul",[e("li",[a._v("针对通用搜索引擎收录关于食品安全资源库较为不全、重复等问题，难以满足人们越来越重视食品的需求，\n构建一套比较完整的食品安全资源库就显得尤为重要。针对现在比较流行的网络爬虫框架进行分析和选择，在现有框架的\n基础上构建一套适合的网络爬虫系统，选用Scrapy-redis对网站进行分布式爬取，提高爬取速度，选用Simhash算法对爬取的\n资源进行相似度判别，过滤掉相似度高的资源，以此提高资源库的质量。通过Elasticsearch的搜索机制基础上设计了食品安全搜索\n引擎的系统架构，最后通过django+vue构建前后端分离项目实现搜索引擎的页面展示和交互。最后通过实验验证，\n相比于传统的通用搜索引擎，该文设计的食品安全搜索引擎的搜索结果相关性更好，数量更多。可以很好的满足人们对食品安全资讯的\n获取。")]),a._v(" "),e("li",[a._v("关键词：食品安全搜索引擎 Scrapy-redis 分布式爬虫 Elasticsearch Simhash")])]),a._v(" "),e("h2",{attrs:{id:"引言"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#引言"}},[a._v("#")]),a._v(" 引言")]),a._v(" "),e("ul",[e("li",[a._v("近年来，随着经济的发展，人民生活水平质量的不断上升，人们越来越关注食品安全问题。食品安全是消费者购买食品的首要考虑因素。\n从经济学角度考虑食品市场的信息不对称是导致食品安全问题产生的根本原因。所以，有关食品安全信息的搜索和获取已成为人们亟待解决\n的重要问题。")]),a._v(" "),e("li",[a._v("为了解决现阶段这一问题，该文对网络搜索引擎的核心思想，以及关键技术等做出了分析，结合URL去重策略，文本去重策略等\n多门技术，提出了一个基于Scrapy-redis和Elasticserch全问搜索引擎的食品安全搜索引擎的设计方案，并使用vue+Django框架实现页面前后端\n的搭建。")])]),a._v(" "),e("h2",{attrs:{id:"_1-搜索引擎中的关键技术研究"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-搜索引擎中的关键技术研究"}},[a._v("#")]),a._v(" 1.搜索引擎中的关键技术研究")]),a._v(" "),e("h3",{attrs:{id:"_1-1-系统总体架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-系统总体架构"}},[a._v("#")]),a._v(" 1.1 系统总体架构")]),a._v(" "),e("ul",[e("li",[a._v("本系统主要分为四个模块。分别是：爬虫系统，搜索模块，API模块，数据可视化模块，如下图所示：\n"),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:s(359),alt:"avatar"}})]),a._v(" "),e("div",{attrs:{align:"center","font-size":"12px"}},[a._v("图1 系统总体架构")])]),a._v(" "),e("li",[a._v("系统通过爬取特定的URL，下载其标识的资源页面，从互联网中获取所爬取的信息。并在爬虫中进行文本的清洗，判重，去重等操作，\n最后将文本存入Mysql数据库中，API则根据可视化要求，通过对Mysql数据库的增删改查操作，实现对应API功能，供前端\n可视化界面调用，从而实现相关功能。")])]),a._v(" "),e("h3",{attrs:{id:"_1-2-爬虫系统"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-爬虫系统"}},[a._v("#")]),a._v(" 1.2 爬虫系统")]),a._v(" "),e("h4",{attrs:{id:"_1-2-1-网络爬虫技术"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-1-网络爬虫技术"}},[a._v("#")]),a._v(" 1.2.1 网络爬虫技术")]),a._v(" "),e("ul",[e("li",[a._v("网络爬虫，是一种能够根据程序设计者所涉及的规则，自动爬取特定网页信息。网络爬虫是搜索引擎的重要组成成分，其主要作用\n是不断下载网页内容镜像到本地，网络爬虫的工作流程如下：")]),a._v(" "),e("li",[a._v("1.解析初始URL域名，得到网页主机地址")]),a._v(" "),e("li",[a._v("2.访问网页，下载网页内容镜像到本地资源库，下载完成之后将当前URL存入到已抓取的队列中")]),a._v(" "),e("li",[a._v("3.对新的url地址进行分类，已访问的地址直接丢弃，未访问的url存入待下载队列")])]),a._v(" "),e("h4",{attrs:{id:"_1-2-2-scrapy爬虫框架"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-2-scrapy爬虫框架"}},[a._v("#")]),a._v(" 1.2.2 Scrapy爬虫框架")]),a._v(" "),e("ul",[e("li",[a._v("Scrapy框架是基于twisted框架的非阻塞异步I/O处理的python事件网络框架，Scrapy主要包含引擎(Scrapy Engine)、调度器(Scheduler)、\n下载器(Downloader)、爬虫(Spiders)、项目管道(Pipeline)、下载器中间件(Downloader Middlewares)、爬虫中间件(Spider Middlewares)、\n调度中间件(Scheduler Middlewares)。\n"),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:s(360),alt:"avatar"}})]),a._v(" "),e("div",{attrs:{align:"center","font-size":"12px"}},[a._v("图2 Scrapy架构")])]),a._v(" "),e("li",[a._v("简单的叙述Scrapy的运行流程为：Engine是整个框架的“大脑”，控制爬虫的运行。Spider通过自定义设置控制Engine向调度器发送指令，调度器\n得到指令后处理后发送请求给下载器，下载器根据接受的请求从互联网下载相关镜像到本地，在传递给spiders，spiders对数据进行处理，之后将数据\n传入项目管道保存或输出，获取的URL发送到Engine进行验证处理，已爬取的URL会被丢掉，未爬取的URL排序后传递到调度器待下载队列，准备下一步\n抓取。只要调度器不为空，爬虫将一直运行下去。")])]),a._v(" "),e("h3",{attrs:{id:"_1-3-搜索模块"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-搜索模块"}},[a._v("#")]),a._v(" 1.3.搜索模块")]),a._v(" "),e("h4",{attrs:{id:"_1-3-1-elasticsearch全文搜索引擎"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-1-elasticsearch全文搜索引擎"}},[a._v("#")]),a._v(" 1.3.1 Elasticsearch全文搜索引擎")]),a._v(" "),e("ul",[e("li",[a._v("Elasticsearch是基于Lucene的开源搜索服务器，它提供了一个分布式多语言多用户的全文搜索引擎。")]),a._v(" "),e("li",[a._v("如果我们建立一个网站或应用程序，并要添加搜索功能，但是想要完成搜索工作的创建是非常困难的。我们希望搜索解决方案要运行速度快，我们\n希望能有一个零配置和一个完全免费的搜索模式，我们希望能够简单地使用JSON通过HTTP来索引数据，我们希望我们的搜索服务器始终可用，我们\n希望能够从一台开始并扩展到数百台，我们希望实时搜索，我们希望简单的多租户，我们希望建立一个云的解决方案。因此我们利用Elasticsearch来解决\n所有这些问题及可能出现的更多其它问题。")]),a._v(" "),e("li",[a._v("Elasticsearch既有数据储存，又有数据分析，是当前流行的企业级搜索引擎，目前联想，微软，Facebook等使用的是elasticsearch作为搜索引擎。")])]),a._v(" "),e("h4",{attrs:{id:"_1-3-2-elasticsearch搜索原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-2-elasticsearch搜索原理"}},[a._v("#")]),a._v(" 1.3.2 Elasticsearch搜索原理")]),a._v(" "),e("ul",[e("li",[a._v("要想了解Elasticsearch的搜索原理，必须先了解几个概念：")]),a._v(" "),e("li",[e("ol",[e("li",[a._v("索引，类型，文档。在Elasticsearch中，索引是存放数据的地方，类型是用来定义数据结构的，而文档就是最终数据了，简单的理解就是\n索引相当于mysql的一个数据库，类型相当于mysql中的表，而文档相当于一条记录，也就是一行。")])])]),a._v(" "),e("li",[e("ol",{attrs:{start:"2"}},[e("li",[a._v("倒排索引(反向索引)，顾名思义，既然有倒排索引就也有正向索引，所谓正向索引一般通过key，去找value。倒排索引就是从词的关键字，去找文本。\n而在搜索引擎中，每个文件对应一个文件ID，文件内容被表示为一系列关键词的集合。通过倒排索引，可以根据词快速获取包含这个词的文档列表。")])])]),a._v(" "),e("li",[a._v("搜索原理简单解析：假设我们有一篇文章，文章中有标题，作者，内容等字段，那么首先，我们可以建立一个名为Articles的索引，然后创建一个名为Article\n的类型，然后通过Mapping来定义每个字段的类型，比如标题，作者定义为Keyword类型(此类型适用于索引结构化字段，只能通过精确值搜索得到)，内容定义为Text类型\n(此字段内容会被分析生成倒排索引，适用于全文搜索)，然后就是把数据转换成JSON格式存放在文档里。在搜索时，根据用户输入的句子，会先经过分词器分词，然后建立\n倒排索引找到相对应文档。")])]),a._v(" "),e("h2",{attrs:{id:"_2-食品安全搜索引擎的设计与实现"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-食品安全搜索引擎的设计与实现"}},[a._v("#")]),a._v(" 2 食品安全搜索引擎的设计与实现")]),a._v(" "),e("h3",{attrs:{id:"_2-1-scrapy-redis分布式爬虫的设计与实现"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-scrapy-redis分布式爬虫的设计与实现"}},[a._v("#")]),a._v(" 2.1 Scrapy-Redis分布式爬虫的设计与实现")]),a._v(" "),e("ul",[e("li",[a._v("Scrapy-redis是使用Scrapy框架与Redis数据库实现网络分布式爬取的一个开源项目，其分布式体现在对各Spider同时爬取打乱的requests、stats数据，存入\n统一的Redis队列，然后通过dupefilters对URL访问去重，再分配给各爬虫进行爬取。本次爬虫模块主要基于Scrapy-redis的分布式网络爬虫的基础上进行\n扩展和实现，包括scrapy-redis爬虫模块，URL去重模块和文本相似度过滤模块。")])]),a._v(" "),e("h4",{attrs:{id:"_2-1-1-scrapy-redis分布式爬虫模块"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-1-scrapy-redis分布式爬虫模块"}},[a._v("#")]),a._v(" 2.1.1 Scrapy-redis分布式爬虫模块")]),a._v(" "),e("ul",[e("li",[a._v("为了测试分布式爬虫功能，爬取的目标选择了食品伙伴网(http://www.foodmate.net/)，网络爬虫从初始地址获得第一批网页链接，此次测试\n使用的是CrawlSpider类实现全站爬取，根据CrawlSpider中定义的目标网页的rules判断链接是否为需要爬取的链接，然后加入待下载队列，进行\nurl去重和排序后交给爬虫爬取，否则就为无用链接丢掉。\n"),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:s(361),alt:"avatar"}})]),a._v(" "),e("div",{attrs:{align:"center","font-size":"12px"}},[a._v("图3 网页链接处理流程")])]),a._v(" "),e("li",[a._v("爬虫相关字段的提取：使用Xpath正则表达式区匹配需要抓取的字段数据，例如")]),a._v(" "),e("li",[a._v("文章的标题title = response.xpath(\"//h1[@id='title']/text()\").get()")]),a._v(" "),e("li",[a._v("文章内容content = response.xpath(\"//div[@id='content']//text()\").getall()")]),a._v(" "),e("li",[a._v("等等这里就只做个示范")])]),a._v(" "),e("h4",{attrs:{id:"_2-1-2-url去重模块"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-2-url去重模块"}},[a._v("#")]),a._v(" 2.1.2 URL去重模块")]),a._v(" "),e("ul",[e("li",[a._v("在进行数据爬取时，若不进行URL去重，不仅会降低爬虫的效率，还会造成储存资源的浪费，Scrapy框架默认的URL去重方法由dupefilters去重器的RFPDupeFilter\n类实现，RFPDupeFilter类会对每一个请求生成信息指纹fp，但是这种URL去重方法比较耗费内存，采用布隆过滤器就可以很好的改进这个问题，但布隆过滤器有时会误判，\n将不重复的URL过滤掉。相比于互联网资源，食品安全的信息条目较少，所以采用默认的URL去重方法。")])]),a._v(" "),e("h4",{attrs:{id:"_2-1-3-文本相似度过滤模块"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-3-文本相似度过滤模块"}},[a._v("#")]),a._v(" 2.1.3 文本相似度过滤模块")]),a._v(" "),e("ul",[e("li",[a._v("文本去重模块是基于SimHash算法，通过调用jieba分词的分词方法，对字段content进行分词，运用TF-IDF算法从中提取带有权重的关键词列表，然后将得到的列表\n交给simhash方法中。在simhash方法中，实现了SimHash算法里的流程，进而得到该文本64位二进制SimHash值，通过比较两个文本SimHash值的海明距离，进而判断两个文本的\n相似度，一般海明距离小于等于3，则为相似。在此项目中，为了不影响爬虫的爬取速度，将去重模块运用在将爬取到的数据存入redis数据库之后，\n经过判重模块清洗数据之后，将清洗过的存入mysql数据库中。\n"),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:s(287),alt:"avatar"}})]),a._v(" "),e("div",{attrs:{align:"center","font-size":"12px"}},[a._v("图4 SimHash算法流程图")])])]),a._v(" "),e("h3",{attrs:{id:"_2-2-基于elasticsearch的搜索引擎的设计与实现"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-基于elasticsearch的搜索引擎的设计与实现"}},[a._v("#")]),a._v(" 2.2 基于Elasticsearch的搜索引擎的设计与实现")]),a._v(" "),e("ul",[e("li",[a._v("在网页的爬虫系统中，已经从页面中获得返回的item对象，此时需要用Elasticsearch-dsl在pipeline中将爬取到的数据保存到Elasticsearch\n中。此时Elasticsearch主要有两大任务：索引文档和搜索文档，索引文档是将爬虫爬取到的数据进行分词，然后构建倒排索引，搜索文档，是根据\n用户输入关键字匹配倒排索引中的文档。")]),a._v(" "),e("li",[a._v("在本次项目中用到的是Elasticsearch-RTF，它是针对中文的一个发行版，并且它用到了很多测试好了的插件，例如ik插件提供的ik_max_word中文分词器\n可以对文本进行最细致的分词。")]),a._v(" "),e("li",[a._v("此次项目中还需要完成搜索建议，通过Elasticsearch搜索建议接口Completion Suggester实现自动补全建议，在使用接口时需要用到搜索建议词，\n故通过analyzer接口可以生成对应建议词。")]),a._v(" "),e("li",[a._v("当用户在搜索界面输入需要查询的关键词，进行搜索时，搜索结果页面会显示搜索内容，搜索热词。处理用户搜索的核心思想是：当点击搜索时会在vue前端\n框架search组件中调用handleSelect方法，通过GET方法将用户输入的搜索内容通过参数传回，并赋值给key_words,并向后台发起用户搜索请求，后台处理用户\n搜索的URL，将返回JSON数据接口，供前端调用。用户处理关键搜索流程如下所示：\n"),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:s(362),alt:"avatar"}})]),a._v(" "),e("div",{attrs:{align:"center","font-size":"12px"}},[a._v("图5 搜索流程图")])])]),a._v(" "),e("h2",{attrs:{id:"_3-api模块和数据可视化模块"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-api模块和数据可视化模块"}},[a._v("#")]),a._v(" 3 API模块和数据可视化模块")]),a._v(" "),e("ul",[e("li",[a._v("API模块和数据可视化模块分别使用Django框架和Vue框架进行开发的。Django框架为数据可视化模块提供API接口，它的主要功能就是接收从客户端发来的\nhttp请求，并从中提取安全有效的参数，在数据库完成数据的增删改查，并将有效的数据返回给客户端。Vue框架开发前端可视化模块主要是完成数据的渲染和给\n后端发送http请。")]),a._v(" "),e("li",[a._v("Django是一个由python写成web应用框架，主要采用MVC的架构模式。在Django框架中，还包含许多强大的第三方插件，具有较强的可扩展性。且使用方便，易于创建\n易维护。")]),a._v(" "),e("li",[a._v("Vue是一套用于构建用户界面的渐进式框架，是一种采用MVVM的架构模式，自底向上增量开发的设计。使用vue作为可视化模块开发的重要原因是，此次选用Vue的目的是\n可以在此基础上将系统扩展，实现更多功能，且易于维护。")])]),a._v(" "),e("h2",{attrs:{id:"_4-系统测试"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-系统测试"}},[a._v("#")]),a._v(" 4 系统测试")]),a._v(" "),e("ul",[e("li",[a._v("关于爬取速度方面，经测试，在单机条件下运行，爬虫持续运行无报错，稳定持续爬取数据，30分钟入库数据1341条，平均38条/min。")])])])}),[],!1,null,null,null);t.default=r.exports}}]);